from prism.config import Config, ADDITIVE_ABLATION_BASE_CONFIG

SUBTRACTIVE_ABLATION_BASE_CONFIG = Config(**ADDITIVE_ABLATION_BASE_CONFIG.__dict__)
SUBTRACTIVE_ABLATION_BASE_CONFIG.learning_rate = 0.0001
SUBTRACTIVE_ABLATION_BASE_CONFIG.n_step_returns_length = 3
SUBTRACTIVE_ABLATION_BASE_CONFIG.use_per = True
SUBTRACTIVE_ABLATION_BASE_CONFIG.per_beta_start = 0.5
SUBTRACTIVE_ABLATION_BASE_CONFIG.per_beta_end = 0.5
SUBTRACTIVE_ABLATION_BASE_CONFIG.per_beta_anneal_timesteps = 1
SUBTRACTIVE_ABLATION_BASE_CONFIG.per_alpha = 0.5
SUBTRACTIVE_ABLATION_BASE_CONFIG.use_layer_norm = True
SUBTRACTIVE_ABLATION_BASE_CONFIG.use_e_greedy = False
SUBTRACTIVE_ABLATION_BASE_CONFIG.use_double_q_learning = False
SUBTRACTIVE_ABLATION_BASE_CONFIG.use_target_network = False
SUBTRACTIVE_ABLATION_BASE_CONFIG.use_dqn = False
SUBTRACTIVE_ABLATION_BASE_CONFIG.use_iqn = True
SUBTRACTIVE_ABLATION_BASE_CONFIG.use_ids = True
SUBTRACTIVE_ABLATION_BASE_CONFIG.iqn_n_current_state_quantile_samples = 32
SUBTRACTIVE_ABLATION_BASE_CONFIG.iqn_n_next_state_quantile_samples = 32
SUBTRACTIVE_ABLATION_BASE_CONFIG.iqn_quantile_samples_per_action = 32
SUBTRACTIVE_ABLATION_BASE_CONFIG.iqn_quantile_model_feature_dim = 64
SUBTRACTIVE_ABLATION_BASE_CONFIG.iqn_quantile_model_feature_dim = 256
SUBTRACTIVE_ABLATION_BASE_CONFIG.iqn_quantile_model_layers = 1
SUBTRACTIVE_ABLATION_BASE_CONFIG.ids_n_q_heads = 10
SUBTRACTIVE_ABLATION_BASE_CONFIG.ids_ensemble_variation_coef = 1e-6

# This says 2 because it builds both the input and output linear layers, separated by a ReLU, so it is
# actually building each Q head with one hidden layer connected to the output layer.
SUBTRACTIVE_ABLATION_BASE_CONFIG.ids_n_q_head_model_layers = 2
SUBTRACTIVE_ABLATION_BASE_CONFIG.ids_q_head_feature_dim = 256
